{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-29T07:55:57.639275Z",
     "iopub.status.busy": "2020-11-29T07:55:57.638612Z",
     "iopub.status.idle": "2020-11-29T07:56:05.654175Z",
     "shell.execute_reply": "2020-11-29T07:56:05.653241Z"
    },
    "papermill": {
     "duration": 8.084006,
     "end_time": "2020-11-29T07:56:05.654333",
     "exception": false,
     "start_time": "2020-11-29T07:55:57.570327",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.metrics import fbeta_score\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:05.781568Z",
     "iopub.status.busy": "2020-11-29T07:56:05.780235Z",
     "iopub.status.idle": "2020-11-29T07:56:05.848729Z",
     "shell.execute_reply": "2020-11-29T07:56:05.848035Z"
    },
    "papermill": {
     "duration": 0.135674,
     "end_time": "2020-11-29T07:56:05.848839",
     "exception": false,
     "start_time": "2020-11-29T07:56:05.713165",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path += ['/kaggle/input/iterstrat', '/kaggle/input/tabnet']\n",
    "\n",
    "from ml_stratifiers import MultilabelStratifiedKFold\n",
    "from tabnet.stacked_tabnet import StackedTabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:05.934188Z",
     "iopub.status.busy": "2020-11-29T07:56:05.933523Z",
     "iopub.status.idle": "2020-11-29T07:56:05.937639Z",
     "shell.execute_reply": "2020-11-29T07:56:05.937014Z"
    },
    "papermill": {
     "duration": 0.049851,
     "end_time": "2020-11-29T07:56:05.937730",
     "exception": false,
     "start_time": "2020-11-29T07:56:05.887879",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_random_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "set_random_seeds(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:06.023528Z",
     "iopub.status.busy": "2020-11-29T07:56:06.022870Z",
     "iopub.status.idle": "2020-11-29T07:56:13.150127Z",
     "shell.execute_reply": "2020-11-29T07:56:13.149466Z"
    },
    "papermill": {
     "duration": 7.17411,
     "end_time": "2020-11-29T07:56:13.150236",
     "exception": false,
     "start_time": "2020-11-29T07:56:05.976126",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_full = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\n",
    "Y_scored = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n",
    "Y_nonscored = pd.read_csv('/kaggle/input/lish-moa/train_targets_nonscored.csv')\n",
    "drug = pd.read_csv('/kaggle/input/lish-moa/train_drug.csv')\n",
    "\n",
    "X_test_full = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n",
    "sig_id = X_test_full['sig_id'].values.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:13.238712Z",
     "iopub.status.busy": "2020-11-29T07:56:13.237680Z",
     "iopub.status.idle": "2020-11-29T07:56:13.241202Z",
     "shell.execute_reply": "2020-11-29T07:56:13.240376Z"
    },
    "papermill": {
     "duration": 0.051197,
     "end_time": "2020-11-29T07:56:13.241355",
     "exception": false,
     "start_time": "2020-11-29T07:56:13.190158",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols = ['cp_type', 'cp_time', 'cp_dose']\n",
    "numerical_cols = [c for c in X_full.columns if c not in ['sig_id'] + cat_cols]\n",
    "label_cols = [c for c in Y_scored.columns if c != 'sig_id']\n",
    "\n",
    "num_features = X_full.shape[1] - 1\n",
    "num_labels = Y_scored.shape[1] - 1\n",
    "num_nonscored_labels = Y_nonscored.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:13.370768Z",
     "iopub.status.busy": "2020-11-29T07:56:13.367971Z",
     "iopub.status.idle": "2020-11-29T07:56:13.374625Z",
     "shell.execute_reply": "2020-11-29T07:56:13.375118Z"
    },
    "papermill": {
     "duration": 0.074055,
     "end_time": "2020-11-29T07:56:13.375264",
     "exception": false,
     "start_time": "2020-11-29T07:56:13.301209",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logloss(Y_true, Y_preds, label_smoothing=0, remove_unpredicted=False):\n",
    "    if remove_unpredicted:\n",
    "        predicted = (Y_preds.sum(axis=1) != 0)\n",
    "        Y_true = Y_true.loc[predicted, :]\n",
    "        Y_preds = Y_preds.loc[predicted, :]\n",
    "    Y_true = Y_true.astype(np.float32)\n",
    "    return tf.reduce_mean(keras.losses.binary_crossentropy(Y_true, Y_preds, label_smoothing=label_smoothing)).numpy()\n",
    "\n",
    "\n",
    "def metrics(Y_true, Y_preds):\n",
    "    pr = tf.keras.metrics.Precision()\n",
    "    pr.update_state(Y_true, Y_preds)\n",
    "    re = tf.keras.metrics.Recall()\n",
    "    re.update_state(Y_true, Y_preds)\n",
    "\n",
    "    pr = pr.result().numpy()\n",
    "    re = re.result().numpy()\n",
    "    f1 = 2 * (pr * re) / (pr + re)\n",
    "    \n",
    "    auc = tf.keras.metrics.AUC(curve='PR', multi_label=True)\n",
    "    auc.update_state(Y_true, Y_preds)\n",
    "    return pr, re, f1, auc.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:13.465728Z",
     "iopub.status.busy": "2020-11-29T07:56:13.465043Z",
     "iopub.status.idle": "2020-11-29T07:56:13.468499Z",
     "shell.execute_reply": "2020-11-29T07:56:13.468956Z"
    },
    "papermill": {
     "duration": 0.053024,
     "end_time": "2020-11-29T07:56:13.469086",
     "exception": false,
     "start_time": "2020-11-29T07:56:13.416062",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_outputs(Y, Y_preds, X=None, idx_list=None, col=None):\n",
    "    if X is not None:\n",
    "        if col is None:\n",
    "            Y.loc[X['cp_type'] == 'trt_cp', :] += Y_preds\n",
    "        else:\n",
    "            Y.loc[X['cp_type'] == 'trt_cp', Y.columns[col]] += Y_preds\n",
    "    elif idx_list is not None:\n",
    "        if col is None:\n",
    "            Y.iloc[idx_list, :] += Y_preds\n",
    "        else:\n",
    "            Y.iloc[idx_list, col] += Y_preds\n",
    "    else:\n",
    "        raise Exception(\"Must provide X or idx_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:13.561820Z",
     "iopub.status.busy": "2020-11-29T07:56:13.558608Z",
     "iopub.status.idle": "2020-11-29T07:56:13.565960Z",
     "shell.execute_reply": "2020-11-29T07:56:13.565431Z"
    },
    "papermill": {
     "duration": 0.056627,
     "end_time": "2020-11-29T07:56:13.566070",
     "exception": false,
     "start_time": "2020-11-29T07:56:13.509443",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance_class_weights(Y_true, class_weights=(0.7, 1.6)):\n",
    "    n_labels = Y_true.shape[1]\n",
    "    weights = np.empty([n_labels, 2])\n",
    "    for i in range(n_labels):\n",
    "        if class_weights == 'balanced':\n",
    "            total = len(Y_true[:, i])\n",
    "            pos = np.sum(Y_true[:, i])\n",
    "            neg = total - pos\n",
    "            weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "            weight_for_1 = (1 / pos)*(total)/2.0\n",
    "            weights[i] = [weight_for_0, weight_for_1]\n",
    "        else:\n",
    "            weights[i] = np.array(class_weights)\n",
    "    return weights\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(weights, label_smoothing=0):\n",
    "    def weighted_loss(Y_true, Y_pred):\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing)\n",
    "        return K.mean(\n",
    "            (weights[:, 0]**(1 - Y_true)) * (weights[:, 1]**(Y_true)) * bce(Y_true, Y_pred),\n",
    "            axis=-1\n",
    "        )\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:13.664662Z",
     "iopub.status.busy": "2020-11-29T07:56:13.663853Z",
     "iopub.status.idle": "2020-11-29T07:56:13.667952Z",
     "shell.execute_reply": "2020-11-29T07:56:13.667151Z"
    },
    "papermill": {
     "duration": 0.061568,
     "end_time": "2020-11-29T07:56:13.668071",
     "exception": false,
     "start_time": "2020-11-29T07:56:13.606503",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_session():\n",
    "    curr_session = tf.compat.v1.get_default_session()\n",
    "    if curr_session is not None:\n",
    "        curr_session.close()\n",
    "    K.clear_session()\n",
    "    \n",
    "    s = tf.compat.v1.InteractiveSession()\n",
    "    tf.compat.v1.keras.backend.set_session(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04595,
     "end_time": "2020-11-29T07:56:13.762690",
     "exception": false,
     "start_time": "2020-11-29T07:56:13.716740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:13.857844Z",
     "iopub.status.busy": "2020-11-29T07:56:13.857148Z",
     "iopub.status.idle": "2020-11-29T07:56:13.860272Z",
     "shell.execute_reply": "2020-11-29T07:56:13.860770Z"
    },
    "papermill": {
     "duration": 0.057107,
     "end_time": "2020-11-29T07:56:13.860923",
     "exception": false,
     "start_time": "2020-11-29T07:56:13.803816",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(df, targets=None):\n",
    "    df = df.copy()\n",
    "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    df = df.drop(columns='sig_id')\n",
    "    \n",
    "    if targets is not None:\n",
    "        targets = targets.copy()\n",
    "        targets = targets.drop(columns='sig_id')\n",
    "        targets = targets[df['cp_type'] == 0].reset_index(drop=True)\n",
    "\n",
    "    df = df[df['cp_type'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    if targets is None:\n",
    "        return df\n",
    "    return df, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:13.953014Z",
     "iopub.status.busy": "2020-11-29T07:56:13.952373Z",
     "iopub.status.idle": "2020-11-29T07:56:13.955910Z",
     "shell.execute_reply": "2020-11-29T07:56:13.955262Z"
    },
    "papermill": {
     "duration": 0.053667,
     "end_time": "2020-11-29T07:56:13.956017",
     "exception": false,
     "start_time": "2020-11-29T07:56:13.902350",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def complete_train_labels(Y, train_idx, test_idx):\n",
    "    \"\"\" Make sure the train set has at least 1 positive class for every label \"\"\"\n",
    "    train_idx, test_idx = list(train_idx), list(test_idx)\n",
    "    Y_train = Y.iloc[train_idx, :]\n",
    "    \n",
    "    train_pos_cnts = Y_train.sum(axis=0)\n",
    "    missing_labels = train_pos_cnts[train_pos_cnts == 0].index\n",
    "    removed_from_test = []\n",
    "    \n",
    "    for l in missing_labels:\n",
    "        for i in test_idx:\n",
    "            if Y[l].iloc[i] == 1:\n",
    "                train_idx.append(i)\n",
    "                removed_from_test.append(i)\n",
    "                break\n",
    "    test_idx = [i for i in test_idx if i not in removed_from_test]\n",
    "    return train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:14.045862Z",
     "iopub.status.busy": "2020-11-29T07:56:14.045189Z",
     "iopub.status.idle": "2020-11-29T07:56:14.380460Z",
     "shell.execute_reply": "2020-11-29T07:56:14.379783Z"
    },
    "papermill": {
     "duration": 0.383064,
     "end_time": "2020-11-29T07:56:14.380568",
     "exception": false,
     "start_time": "2020-11-29T07:56:13.997504",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = preprocess(X_full, Y_scored)\n",
    "X_test = preprocess(X_test_full)\n",
    "Y_train_nonscored = Y_nonscored.drop(columns='sig_id')[X_full['cp_type'] == 'trt_cp'].reset_index(drop=True)\n",
    "initial_bias = -np.log(Y_train.mean(axis=0).values)\n",
    "nonscored_initial_bias = -np.log(Y_train_nonscored.mean(axis=0).values + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:14.477190Z",
     "iopub.status.busy": "2020-11-29T07:56:14.472130Z",
     "iopub.status.idle": "2020-11-29T07:56:14.492197Z",
     "shell.execute_reply": "2020-11-29T07:56:14.491513Z"
    },
    "papermill": {
     "duration": 0.070558,
     "end_time": "2020-11-29T07:56:14.492351",
     "exception": false,
     "start_time": "2020-11-29T07:56:14.421793",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multilabel_split_by_drugs(scored, n_folds, seed):\n",
    "    scored = scored.copy().merge(drug, on='sig_id', how='left') \n",
    "\n",
    "    # LOCATE DRUGS\n",
    "    vc = scored.drug_id.value_counts()\n",
    "    vc1 = vc.loc[vc <= 18].index\n",
    "    vc2 = vc.loc[vc > 18].index\n",
    "\n",
    "    # STRATIFY DRUGS 18X OR LESS\n",
    "    dct1 = {}\n",
    "    dct2 = {}\n",
    "    skf = MultilabelStratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    tmp = scored.groupby('drug_id')[label_cols].mean().loc[vc1]\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(tmp, tmp[label_cols])):\n",
    "        dd = {k:fold for k in tmp.index[test_idx].values}\n",
    "        dct1.update(dd)\n",
    "\n",
    "    # STRATIFY DRUGS MORE THAN 18X\n",
    "    skf = MultilabelStratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(tmp, tmp[label_cols])):\n",
    "        dd = {k:fold for k in tmp.sig_id[test_idx].values}\n",
    "        dct2.update(dd)\n",
    "\n",
    "    # ASSIGN FOLDS\n",
    "    scored['fold'] = scored.drug_id.map(dct1)\n",
    "    scored.loc[scored.fold.isna(), 'fold'] = scored.loc[scored.fold.isna(), 'sig_id'].map(dct2)\n",
    "    scored.fold = scored.fold.astype('int8')\n",
    "    \n",
    "    test_idx = [np.where(scored['fold'] == fold)[0].tolist() for fold in range(n_folds)]\n",
    "    train_idx = [np.where(scored['fold'] != fold)[0].tolist() for fold in range(n_folds)]\n",
    "    return zip(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:14.634343Z",
     "iopub.status.busy": "2020-11-29T07:56:14.629090Z",
     "iopub.status.idle": "2020-11-29T07:56:14.667932Z",
     "shell.execute_reply": "2020-11-29T07:56:14.667250Z"
    },
    "papermill": {
     "duration": 0.114781,
     "end_time": "2020-11-29T07:56:14.668042",
     "exception": false,
     "start_time": "2020-11-29T07:56:14.553261",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_features(X):\n",
    "    return X.iloc[:, [\n",
    "        1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
    "        18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
    "        32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
    "        47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
    "        61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
    "        74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
    "        89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
    "       102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
    "       115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
    "       129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
    "       144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
    "       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
    "       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
    "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
    "       198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
    "       213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
    "       227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
    "       240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
    "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
    "       267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
    "       281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
    "       295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
    "       310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
    "       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
    "       337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
    "       350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
    "       363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
    "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
    "       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
    "       405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
    "       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
    "       432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
    "       447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
    "       463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
    "       476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
    "       490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
    "       506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
    "       522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
    "       538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
    "       552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
    "       571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
    "       586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
    "       600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
    "       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
    "       631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
    "       645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
    "       660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
    "       673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
    "       686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
    "       701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
    "       718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
    "       733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
    "       748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
    "       762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
    "       775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
    "       789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
    "       804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
    "       821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
    "       837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
    "       854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
    "       870, 871, 872, 873, 874\n",
    "    ]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:14.759680Z",
     "iopub.status.busy": "2020-11-29T07:56:14.758611Z",
     "iopub.status.idle": "2020-11-29T07:56:14.761476Z",
     "shell.execute_reply": "2020-11-29T07:56:14.761944Z"
    },
    "papermill": {
     "duration": 0.052114,
     "end_time": "2020-11-29T07:56:14.762081",
     "exception": false,
     "start_time": "2020-11-29T07:56:14.709967",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gene_cols = [c for c in X_train.columns if c.startswith('g-')]\n",
    "cell_cols = [c for c in X_train.columns if c.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:14.857174Z",
     "iopub.status.busy": "2020-11-29T07:56:14.856487Z",
     "iopub.status.idle": "2020-11-29T07:56:23.613680Z",
     "shell.execute_reply": "2020-11-29T07:56:23.612790Z"
    },
    "papermill": {
     "duration": 8.808432,
     "end_time": "2020-11-29T07:56:23.613817",
     "exception": false,
     "start_time": "2020-11-29T07:56:14.805385",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "qtransform = joblib.load('/kaggle/input/qtransform/qtransform.joblib')\n",
    "\n",
    "X_train_selected = select_features(X_train)\n",
    "X_test_selected = select_features(X_test)\n",
    "X_train_qtrans = np.concatenate([X_train_selected[:, :2], qtransform.transform(X_train_selected[:, 2:])], axis=1)\n",
    "X_test_qtrans =  np.concatenate([X_test_selected[:, :2], qtransform.transform(X_test_selected[:, 2:])], axis=1)\n",
    "\n",
    "pca_qtrans_2 = joblib.load('/kaggle/input/qtransform/pca_qtrans.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:23.705565Z",
     "iopub.status.busy": "2020-11-29T07:56:23.704897Z",
     "iopub.status.idle": "2020-11-29T07:56:23.753212Z",
     "shell.execute_reply": "2020-11-29T07:56:23.752480Z"
    },
    "papermill": {
     "duration": 0.096928,
     "end_time": "2020-11-29T07:56:23.753404",
     "exception": false,
     "start_time": "2020-11-29T07:56:23.656476",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "std_scaler_2 = joblib.load('/kaggle/input/qtransform/std_scaler_2.joblib')\n",
    "gene_pca_2 = joblib.load('/kaggle/input/qtransform/gene_pca_2.joblib')\n",
    "cell_pca_2 = joblib.load('/kaggle/input/qtransform/cell_pca_2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:23.881168Z",
     "iopub.status.busy": "2020-11-29T07:56:23.880369Z",
     "iopub.status.idle": "2020-11-29T07:56:24.010790Z",
     "shell.execute_reply": "2020-11-29T07:56:24.010037Z"
    },
    "papermill": {
     "duration": 0.195563,
     "end_time": "2020-11-29T07:56:24.010946",
     "exception": false,
     "start_time": "2020-11-29T07:56:23.815383",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_g = joblib.load('/kaggle/input/qtransform/kmeans_g.joblib')\n",
    "kmeans_c = joblib.load('/kaggle/input/qtransform/kmeans_c.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.061542,
     "end_time": "2020-11-29T07:56:24.134260",
     "exception": false,
     "start_time": "2020-11-29T07:56:24.072718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:24.262779Z",
     "iopub.status.busy": "2020-11-29T07:56:24.262110Z",
     "iopub.status.idle": "2020-11-29T07:56:24.266177Z",
     "shell.execute_reply": "2020-11-29T07:56:24.265545Z"
    },
    "papermill": {
     "duration": 0.070637,
     "end_time": "2020-11-29T07:56:24.266281",
     "exception": false,
     "start_time": "2020-11-29T07:56:24.195644",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOAD_PRETRAINED_NN = True\n",
    "REUSE_SPLITS_NN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:24.357432Z",
     "iopub.status.busy": "2020-11-29T07:56:24.356761Z",
     "iopub.status.idle": "2020-11-29T07:56:24.360462Z",
     "shell.execute_reply": "2020-11-29T07:56:24.359940Z"
    },
    "papermill": {
     "duration": 0.052029,
     "end_time": "2020-11-29T07:56:24.360568",
     "exception": false,
     "start_time": "2020-11-29T07:56:24.308539",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_nn(X):\n",
    "    X_pca = pca_qtrans_2.transform(X)\n",
    "    return np.concatenate([X, X_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:24.449215Z",
     "iopub.status.busy": "2020-11-29T07:56:24.448521Z",
     "iopub.status.idle": "2020-11-29T07:56:24.719434Z",
     "shell.execute_reply": "2020-11-29T07:56:24.718390Z"
    },
    "papermill": {
     "duration": 0.316789,
     "end_time": "2020-11-29T07:56:24.719573",
     "exception": false,
     "start_time": "2020-11-29T07:56:24.402784",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_nn = preprocess_nn(X_train_qtrans)\n",
    "X_test_nn = preprocess_nn(X_test_qtrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:24.820615Z",
     "iopub.status.busy": "2020-11-29T07:56:24.819977Z",
     "iopub.status.idle": "2020-11-29T07:56:25.158695Z",
     "shell.execute_reply": "2020-11-29T07:56:25.158143Z"
    },
    "papermill": {
     "duration": 0.384794,
     "end_time": "2020-11-29T07:56:25.158810",
     "exception": false,
     "start_time": "2020-11-29T07:56:24.774016",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:25.256784Z",
     "iopub.status.busy": "2020-11-29T07:56:25.251748Z",
     "iopub.status.idle": "2020-11-29T07:56:25.289139Z",
     "shell.execute_reply": "2020-11-29T07:56:25.288233Z"
    },
    "papermill": {
     "duration": 0.088234,
     "end_time": "2020-11-29T07:56:25.289274",
     "exception": false,
     "start_time": "2020-11-29T07:56:25.201040",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLabelNN(BaseEstimator):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_input_features=num_features,\n",
    "                 n_output_labels=num_labels,\n",
    "                 n_hidden_units=2048):\n",
    "        self.n_input_features = n_input_features\n",
    "        self.n_output_labels = n_output_labels\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self._model_nn = None\n",
    "        \n",
    "    def _create_nn_model(self, loss_fn=None, loss_fn_aux=None):\n",
    "        output_bias = keras.initializers.Constant(initial_bias)\n",
    "        aux_output_bias = keras.initializers.Constant(nonscored_initial_bias)\n",
    "        \n",
    "        inp = keras.layers.Input(self.n_input_features)\n",
    "        gauss_noise = keras.layers.GaussianNoise(0.001)(inp)\n",
    "        norm_1 = keras.layers.BatchNormalization()(gauss_noise)\n",
    "\n",
    "        dense_1 = tfa.layers.WeightNormalization(keras.layers.Dense(\n",
    "            self.n_hidden_units // 2,\n",
    "            activation='elu',\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=regularizers.l2(1e-5)\n",
    "        ))(norm_1)\n",
    "        norm_2 = keras.layers.BatchNormalization()(dense_1)\n",
    "        dropout_1 = keras.layers.Dropout(0.4)(norm_2)\n",
    "        \n",
    "        dense_2 = tfa.layers.WeightNormalization(keras.layers.Dense(\n",
    "            self.n_hidden_units,\n",
    "            activation='elu',\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=regularizers.l2(1e-5)\n",
    "        ))(dropout_1)\n",
    "        norm_3 = keras.layers.BatchNormalization()(dense_2)\n",
    "        dropout_2 = keras.layers.Dropout(0.4)(norm_3)\n",
    "        \n",
    "        dropout_3 = keras.layers.Dropout(0.5)(norm_2)\n",
    "        dense_3 = tfa.layers.WeightNormalization(keras.layers.Dense(\n",
    "            self.n_hidden_units // 2,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l2(1e-5)\n",
    "        ))(dropout_3)\n",
    "        norm_4 = keras.layers.BatchNormalization()(dense_3)\n",
    "        dropout_4 = keras.layers.Dropout(0.4)(norm_4)\n",
    "\n",
    "        output_1 = tfa.layers.WeightNormalization(keras.layers.Dense(\n",
    "            self.n_output_labels,\n",
    "            activation='sigmoid',\n",
    "            bias_initializer=output_bias\n",
    "        ), name='main')(dropout_4)\n",
    "        output_2 = tfa.layers.WeightNormalization(keras.layers.Dense(\n",
    "            Y_train_nonscored.shape[1],\n",
    "            activation='sigmoid',\n",
    "            bias_initializer=aux_output_bias\n",
    "        ), name='aux')(dropout_2)\n",
    "        \n",
    "        model_nn = keras.models.Model(inputs=inp, outputs=[output_1, output_2])\n",
    "        model_nn.compile(\n",
    "            loss=[\n",
    "                keras.losses.BinaryCrossentropy(label_smoothing=2e-4) if loss_fn is None else loss_fn,\n",
    "                keras.losses.BinaryCrossentropy(label_smoothing=2e-4) if loss_fn_aux is None else loss_fn_aux\n",
    "            ],\n",
    "            optimizer=keras.optimizers.Nadam(learning_rate=0.01),\n",
    "            metrics=['binary_crossentropy'],\n",
    "            loss_weights=[0.9, 0.1]\n",
    "        )\n",
    "        return model_nn\n",
    "        \n",
    "    def fit(self, X, Y, X_valid, Y_valid, Y_aux, Y_aux_valid, max_epochs=100, save_weights=True):\n",
    "        Y = tf.cast(Y, tf.float32)\n",
    "        Y_valid = tf.cast(Y_valid, tf.float32)\n",
    "        Y_aux = tf.cast(Y_aux, tf.float32)\n",
    "        Y_aux_valid = tf.cast(Y_aux_valid, tf.float32)\n",
    "\n",
    "        class_weights = balance_class_weights(Y.numpy(), class_weights=(0.7, 2.0))\n",
    "        loss_fn = weighted_binary_crossentropy(class_weights, label_smoothing=1e-3)\n",
    "        class_weights_aux = balance_class_weights(Y_aux.numpy(), class_weights=(0.7, 1.4))\n",
    "        loss_fn_aux = weighted_binary_crossentropy(class_weights_aux, label_smoothing=1e-3)\n",
    "        self._model_nn = self._create_nn_model(loss_fn=loss_fn, loss_fn_aux=loss_fn_aux)\n",
    "\n",
    "        learning_rate_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            patience=3, verbose=1, factor=0.1, min_lr=1e-8\n",
    "        )\n",
    "        early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.0001, patience=10,\n",
    "            restore_best_weights=False\n",
    "        )\n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "            f'model_nn/weights_{repeat}_{fold}.h5',\n",
    "            monitor='val_main_binary_crossentropy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True\n",
    "        )\n",
    "        callbacks = [early_stop_cb, learning_rate_cb, checkpoint_cb] if save_weights else [early_stop_cb, learning_rate_cb]\n",
    "\n",
    "        history = self._model_nn.fit(\n",
    "            X, [Y, Y_aux],\n",
    "            batch_size=128,\n",
    "            epochs=max_epochs,\n",
    "            validation_data=(X_valid, [Y_valid, Y_aux_valid]),\n",
    "            callbacks=callbacks,\n",
    "            shuffle=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self._model_nn.predict(X)\n",
    "    \n",
    "    def score(self, X, Y_true):\n",
    "        Y_preds = self.predict(X)[0]\n",
    "        return logloss(Y_true, Y_preds)\n",
    "    \n",
    "    def save_weights(self, repeat, fold):\n",
    "        self._model_nn.save_weights('model_nn/weights_{}_{}.h5'.format(repeat, fold))\n",
    "        \n",
    "    def load_weights(self, repeat, fold, pretrained=True):\n",
    "        if self._model_nn is None:\n",
    "            self._model_nn = self._create_nn_model()\n",
    "        if pretrained:\n",
    "            base_path = '/kaggle/input/model-nn/'\n",
    "        else:\n",
    "            base_path = './working/'\n",
    "        self._model_nn.load_weights(base_path + 'model_nn/weights_{}_{}.h5'.format(repeat, fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:25.421339Z",
     "iopub.status.busy": "2020-11-29T07:56:25.420487Z",
     "iopub.status.idle": "2020-11-29T07:56:25.424848Z",
     "shell.execute_reply": "2020-11-29T07:56:25.424036Z"
    },
    "papermill": {
     "duration": 0.07264,
     "end_time": "2020-11-29T07:56:25.424964",
     "exception": false,
     "start_time": "2020-11-29T07:56:25.352324",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 1156, 415, 608, 1993]\n"
     ]
    }
   ],
   "source": [
    "n_repeats = 5\n",
    "n_folds = 7\n",
    "kfold_seeds = [74, 1156, 415, 608, 1993] if REUSE_SPLITS_NN else np.random.randint(42, 1337, n_repeats)\n",
    "kfold_seeds = kfold_seeds[:n_repeats]\n",
    "assert len(kfold_seeds) == n_repeats\n",
    "print(kfold_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2020-11-29T07:56:25.574254Z",
     "iopub.status.busy": "2020-11-29T07:56:25.561668Z",
     "iopub.status.idle": "2020-11-29T08:00:39.124497Z",
     "shell.execute_reply": "2020-11-29T08:00:39.123762Z"
    },
    "papermill": {
     "duration": 253.635821,
     "end_time": "2020-11-29T08:00:39.124622",
     "exception": false,
     "start_time": "2020-11-29T07:56:25.488801",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Repeat 1/5. Fold 1/7\n",
      "Val loss: 0.015600921586155891\n",
      "** Repeat 1/5. Fold 2/7\n",
      "Val loss: 0.015657443553209305\n",
      "** Repeat 1/5. Fold 3/7\n",
      "Val loss: 0.015989556908607483\n",
      "** Repeat 1/5. Fold 4/7\n",
      "Val loss: 0.015861568972468376\n",
      "** Repeat 1/5. Fold 5/7\n",
      "Val loss: 0.01586352474987507\n",
      "** Repeat 1/5. Fold 6/7\n",
      "Val loss: 0.01587575115263462\n",
      "** Repeat 1/5. Fold 7/7\n",
      "Val loss: 0.015924906358122826\n",
      "----------\n",
      "Repeat avg val loss: 0.015824811533093452\n",
      "Repeat OOF val loss: 0.0158248473212246\n",
      "----------\n",
      "** Repeat 2/5. Fold 1/7\n",
      "Val loss: 0.01592746190726757\n",
      "** Repeat 2/5. Fold 2/7\n",
      "Val loss: 0.015704503282904625\n",
      "** Repeat 2/5. Fold 3/7\n",
      "Val loss: 0.015848109498620033\n",
      "** Repeat 2/5. Fold 4/7\n",
      "Val loss: 0.015885770320892334\n",
      "** Repeat 2/5. Fold 5/7\n",
      "Val loss: 0.015708347782492638\n",
      "** Repeat 2/5. Fold 6/7\n",
      "Val loss: 0.015785587951540947\n",
      "** Repeat 2/5. Fold 7/7\n",
      "Val loss: 0.015963325276970863\n",
      "----------\n",
      "Repeat avg val loss: 0.015831872820854187\n",
      "Repeat OOF val loss: 0.01571310308753488\n",
      "----------\n",
      "** Repeat 3/5. Fold 1/7\n",
      "Val loss: 0.01583353616297245\n",
      "** Repeat 3/5. Fold 2/7\n",
      "Val loss: 0.015830613672733307\n",
      "** Repeat 3/5. Fold 3/7\n",
      "Val loss: 0.015848975628614426\n",
      "** Repeat 3/5. Fold 4/7\n",
      "Val loss: 0.015649283304810524\n",
      "** Repeat 3/5. Fold 5/7\n",
      "Val loss: 0.01590988226234913\n",
      "** Repeat 3/5. Fold 6/7\n",
      "Val loss: 0.01566128432750702\n",
      "** Repeat 3/5. Fold 7/7\n",
      "Val loss: 0.01601940393447876\n",
      "----------\n",
      "Repeat avg val loss: 0.015821853652596474\n",
      "Repeat OOF val loss: 0.015668522683748846\n",
      "----------\n",
      "** Repeat 4/5. Fold 1/7\n",
      "Val loss: 0.015867965295910835\n",
      "** Repeat 4/5. Fold 2/7\n",
      "Val loss: 0.015716440975666046\n",
      "** Repeat 4/5. Fold 3/7\n",
      "Val loss: 0.015859972685575485\n",
      "** Repeat 4/5. Fold 4/7\n",
      "Val loss: 0.015831761062145233\n",
      "** Repeat 4/5. Fold 5/7\n",
      "Val loss: 0.016014620661735535\n",
      "** Repeat 4/5. Fold 6/7\n",
      "Val loss: 0.015729611739516258\n",
      "** Repeat 4/5. Fold 7/7\n",
      "Val loss: 0.015786001458764076\n",
      "----------\n",
      "Repeat avg val loss: 0.015829483047127724\n",
      "Repeat OOF val loss: 0.01564844289975039\n",
      "----------\n",
      "** Repeat 5/5. Fold 1/7\n",
      "Val loss: 0.01572570949792862\n",
      "** Repeat 5/5. Fold 2/7\n",
      "Val loss: 0.015923025086522102\n",
      "** Repeat 5/5. Fold 3/7\n",
      "Val loss: 0.01595611497759819\n",
      "** Repeat 5/5. Fold 4/7\n",
      "Val loss: 0.01583518274128437\n",
      "** Repeat 5/5. Fold 5/7\n",
      "Val loss: 0.015596541576087475\n",
      "** Repeat 5/5. Fold 6/7\n",
      "Val loss: 0.015815369784832\n",
      "** Repeat 5/5. Fold 7/7\n",
      "Val loss: 0.015942715108394623\n",
      "----------\n",
      "Repeat avg val loss: 0.015827808529138565\n",
      "Repeat OOF val loss: 0.015638645357592264\n",
      "----------\n",
      "==========\n",
      "Overall avg val loss: 0.01582716591656208\n",
      "Overall OOF val loss: 0.015638645357592264\n"
     ]
    }
   ],
   "source": [
    "val_losses = []\n",
    "histories = []\n",
    "Y_train_preds_nn = pd.DataFrame(np.zeros((X_train.shape[0], num_labels)), columns=label_cols)\n",
    "Y_test_preds_nn = pd.DataFrame(np.zeros((X_test_full.shape[0], num_labels)), columns=label_cols)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "for repeat, kf_seed in enumerate(kfold_seeds):\n",
    "    kf = MultilabelStratifiedKFold(n_splits=n_folds, random_state=kf_seed, shuffle=True)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X_train, Y_train)):\n",
    "        print('** Repeat {}/{}. Fold {}/{}'.format(repeat + 1, n_repeats, fold + 1, n_folds))\n",
    "        K.clear_session()\n",
    "        train_idx, test_idx = complete_train_labels(Y_train, train_idx, test_idx)\n",
    "        train_idx = tf.random.shuffle(train_idx)\n",
    "        X_train_fold = tf.gather(X_train_nn, train_idx, axis=0)\n",
    "        Y_train_fold = tf.gather(Y_train, train_idx, axis=0)\n",
    "        Y_train_aux_fold = tf.gather(Y_train_nonscored, train_idx, axis=0)\n",
    "        X_valid_fold = tf.gather(X_train_nn, test_idx, axis=0)\n",
    "        Y_valid_fold = tf.gather(Y_train, test_idx, axis=0)\n",
    "        Y_valid_aux_fold = tf.gather(Y_train_nonscored, test_idx, axis=0)\n",
    "\n",
    "        model_nn = MultiLabelNN(n_input_features=X_train_nn.shape[1])\n",
    "        if LOAD_PRETRAINED_NN:\n",
    "            model_nn.load_weights(repeat, fold)\n",
    "            val_loss = model_nn.score(X_valid_fold, Y_valid_fold.numpy())\n",
    "        else:\n",
    "            history = model_nn.fit(\n",
    "                X_train_fold, Y_train_fold,\n",
    "                X_valid_fold, Y_valid_fold,\n",
    "                Y_train_aux_fold, Y_valid_aux_fold\n",
    "            ) \n",
    "            histories.append(history)\n",
    "            val_loss = min(history.history['val_main_binary_crossentropy'])\n",
    "            model_nn.load_weights(repeat, fold, pretrained=False)\n",
    "\n",
    "        val_preds = model_nn.predict(X_valid_fold)[0]\n",
    "        add_outputs(Y_train_preds_nn, val_preds, idx_list=test_idx)\n",
    "\n",
    "        test_preds = model_nn.predict(X_test_nn)[0]\n",
    "        add_outputs(Y_test_preds_nn, test_preds, X=X_test_full)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        print('Val loss: {}'.format(val_loss))\n",
    "        \n",
    "        del model_nn\n",
    "\n",
    "    print('----------')\n",
    "    print('Repeat avg val loss: {}'.format(np.mean(val_losses[repeat*n_folds:((repeat + 1)*n_folds)])))\n",
    "    print('Repeat OOF val loss: {}'.format(logloss(Y_train, Y_train_preds_nn / (repeat + 1), remove_unpredicted=True)))\n",
    "    print('----------')\n",
    "\n",
    "Y_train_preds_nn /= n_repeats       \n",
    "Y_test_preds_nn.loc[X_test_full['cp_type'] == 'trt_cp', :] /= (n_folds * n_repeats)\n",
    "print('==========')\n",
    "print('Overall avg val loss: {}'.format(np.mean(val_losses)))\n",
    "print('Overall OOF val loss: {}'.format(logloss(Y_train, Y_train_preds_nn, remove_unpredicted=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:00:39.307056Z",
     "iopub.status.busy": "2020-11-29T08:00:39.306375Z",
     "iopub.status.idle": "2020-11-29T08:00:39.310508Z",
     "shell.execute_reply": "2020-11-29T08:00:39.309966Z"
    },
    "papermill": {
     "duration": 0.098064,
     "end_time": "2020-11-29T08:00:39.310617",
     "exception": false,
     "start_time": "2020-11-29T08:00:39.212553",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !zip -r model_nn.zip model_nn\n",
    "# !rm -r model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:00:39.435829Z",
     "iopub.status.busy": "2020-11-29T08:00:39.435167Z",
     "iopub.status.idle": "2020-11-29T08:00:39.437857Z",
     "shell.execute_reply": "2020-11-29T08:00:39.438339Z"
    },
    "papermill": {
     "duration": 0.06857,
     "end_time": "2020-11-29T08:00:39.438466",
     "exception": false,
     "start_time": "2020-11-29T08:00:39.369896",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    df = pd.DataFrame({\n",
    "        'loss': history.history['main_binary_crossentropy'],\n",
    "        'val_loss': history.history['val_main_binary_crossentropy'],\n",
    "    })\n",
    "    print(df['val_loss'].min())\n",
    "    df = df.loc[3:, :]\n",
    "    df.plot()\n",
    "    plt.axvline(x=df['val_loss'].idxmin(), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:00:39.561911Z",
     "iopub.status.busy": "2020-11-29T08:00:39.561045Z",
     "iopub.status.idle": "2020-11-29T08:00:39.564617Z",
     "shell.execute_reply": "2020-11-29T08:00:39.563946Z"
    },
    "papermill": {
     "duration": 0.067125,
     "end_time": "2020-11-29T08:00:39.564717",
     "exception": false,
     "start_time": "2020-11-29T08:00:39.497592",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_history(histories[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059042,
     "end_time": "2020-11-29T08:00:39.682883",
     "exception": false,
     "start_time": "2020-11-29T08:00:39.623841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:00:39.805452Z",
     "iopub.status.busy": "2020-11-29T08:00:39.804778Z",
     "iopub.status.idle": "2020-11-29T08:00:39.808337Z",
     "shell.execute_reply": "2020-11-29T08:00:39.807699Z"
    },
    "papermill": {
     "duration": 0.066551,
     "end_time": "2020-11-29T08:00:39.808438",
     "exception": false,
     "start_time": "2020-11-29T08:00:39.741887",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOAD_PRETRAINED_TABNET = True\n",
    "REUSE_SPLITS_TABNET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:00:39.933056Z",
     "iopub.status.busy": "2020-11-29T08:00:39.932447Z",
     "iopub.status.idle": "2020-11-29T08:00:40.273704Z",
     "shell.execute_reply": "2020-11-29T08:00:40.273158Z"
    },
    "papermill": {
     "duration": 0.406287,
     "end_time": "2020-11-29T08:00:40.273812",
     "exception": false,
     "start_time": "2020-11-29T08:00:39.867525",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir model_tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:00:40.404761Z",
     "iopub.status.busy": "2020-11-29T08:00:40.401395Z",
     "iopub.status.idle": "2020-11-29T08:00:40.408981Z",
     "shell.execute_reply": "2020-11-29T08:00:40.408331Z"
    },
    "papermill": {
     "duration": 0.075043,
     "end_time": "2020-11-29T08:00:40.409100",
     "exception": false,
     "start_time": "2020-11-29T08:00:40.334057",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_tabnet(X, X_qtrans):\n",
    "    X_pca = pca_qtrans_2.transform(X_qtrans)\n",
    "    g_std = X[gene_cols].std(axis=1).values.reshape(-1, 1)\n",
    "    g_kurt = X[gene_cols].kurtosis(axis=1).values.reshape(-1, 1)\n",
    "    g_skew = X[gene_cols].skew(axis=1).values.reshape(-1, 1)\n",
    "    c_std = X[cell_cols].std(axis=1).values.reshape(-1, 1)\n",
    "    c_kurt = X[cell_cols].kurtosis(axis=1).values.reshape(-1, 1)\n",
    "    c_skew = X[cell_cols].skew(axis=1).values.reshape(-1, 1)\n",
    "    X_std = pd.DataFrame(std_scaler_2.transform(X[numerical_cols]), columns=numerical_cols)\n",
    "    g_cluster = kmeans_g.predict(X_std[gene_cols]).reshape(-1, 1)\n",
    "    c_cluster = kmeans_c.predict(X_std[cell_cols]).reshape(-1, 1)\n",
    "    return np.concatenate([\n",
    "        X_qtrans, X_pca,\n",
    "        g_std, g_kurt, g_skew,\n",
    "        c_std, c_kurt, c_skew,\n",
    "        g_cluster, c_cluster\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:00:40.590822Z",
     "iopub.status.busy": "2020-11-29T08:00:40.590104Z",
     "iopub.status.idle": "2020-11-29T08:00:42.037830Z",
     "shell.execute_reply": "2020-11-29T08:00:42.037190Z"
    },
    "papermill": {
     "duration": 1.540052,
     "end_time": "2020-11-29T08:00:42.037970",
     "exception": false,
     "start_time": "2020-11-29T08:00:40.497918",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tabnet = preprocess_tabnet(X_train, X_train_qtrans)\n",
    "X_test_tabnet = preprocess_tabnet(X_test, X_test_qtrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:00:42.199465Z",
     "iopub.status.busy": "2020-11-29T08:00:42.198697Z",
     "iopub.status.idle": "2020-11-29T08:00:42.217037Z",
     "shell.execute_reply": "2020-11-29T08:00:42.217738Z"
    },
    "papermill": {
     "duration": 0.105253,
     "end_time": "2020-11-29T08:00:42.217911",
     "exception": false,
     "start_time": "2020-11-29T08:00:42.112658",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLabelTabNet(BaseEstimator):\n",
    "\n",
    "    def __init__(self, n_input_features, n_output_labels):\n",
    "        self.n_input_features = n_input_features\n",
    "        self.n_output_labels = n_output_labels\n",
    "        self.estimator_ = None\n",
    "        \n",
    "    def _create_model(self, loss_fn=None, output_bias=None):\n",
    "        if output_bias is not None:\n",
    "            output_bias = keras.initializers.Constant(output_bias)\n",
    "\n",
    "        clf = StackedTabNetClassifier(\n",
    "            feature_columns=None, \n",
    "            num_classes=1024,\n",
    "            num_layers=1,\n",
    "            feature_dim=2048,\n",
    "            output_dim=1024,\n",
    "            num_features=self.n_input_features,\n",
    "            num_decision_steps=1,\n",
    "            relaxation_factor=1.5,\n",
    "            sparsity_coefficient=1e-5,\n",
    "            num_groups=-1,\n",
    "            output_activation='relu'\n",
    "        )\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.InputLayer(self.n_input_features),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            keras.layers.GaussianNoise(0.001),\n",
    "            clf,\n",
    "            keras.layers.Dropout(0.7),\n",
    "            tfa.layers.WeightNormalization(keras.layers.Dense(\n",
    "                self.n_output_labels,\n",
    "                activation='sigmoid',\n",
    "                bias_initializer=output_bias\n",
    "            ))\n",
    "        ])\n",
    "        model.compile(\n",
    "            loss=keras.losses.BinaryCrossentropy(label_smoothing=2e-3) if loss_fn is None else loss_fn,\n",
    "            optimizer=tfa.optimizers.AdamW(weight_decay=1e-5),\n",
    "            metrics=['binary_crossentropy']\n",
    "        )\n",
    "        return model\n",
    "        \n",
    "    def fit(self, X, Y, X_valid, Y_valid, repeat, fold, max_epochs=200):\n",
    "        Y = tf.cast(Y, tf.float32)\n",
    "        Y_valid = tf.cast(Y_valid, tf.float32)\n",
    "        class_weights = balance_class_weights(Y.numpy(), class_weights=(0.5, 15.0))\n",
    "        loss_fn = weighted_binary_crossentropy(class_weights, label_smoothing=2e-3)\n",
    "        self.estimator_ = self._create_model(loss_fn=loss_fn, output_bias=initial_bias)\n",
    "\n",
    "        learning_rate_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', min_delta=1e-5,\n",
    "            patience=3, verbose=1, factor=0.1, min_lr=1e-7\n",
    "        )\n",
    "        early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=1e-5, patience=10, restore_best_weights=False\n",
    "        )\n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "            f'model_tabnet/weights_{repeat}_{fold}.h5',\n",
    "            monitor='val_binary_crossentropy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True\n",
    "        )\n",
    "\n",
    "        history = self.estimator_.fit(\n",
    "            X, Y,\n",
    "            batch_size=128,\n",
    "            epochs=max_epochs,\n",
    "            validation_data=(X_valid, Y_valid),\n",
    "            callbacks=[early_stop_cb, learning_rate_cb, checkpoint_cb],\n",
    "            shuffle=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.estimator_.predict(X)\n",
    "    \n",
    "    def score(self, X, Y_true):\n",
    "        Y_preds = self.predict(X)\n",
    "        return logloss(Y_true, Y_preds)\n",
    "    \n",
    "    def save_weights(self, repeat, fold):\n",
    "        self.estimator_.save_weights('model_tabnet/weights_{}_{}.h5'.format(repeat, fold))\n",
    "        \n",
    "    def load_weights(self, repeat, fold, pretrained=True):\n",
    "        if self.estimator_ is None:\n",
    "            self.estimator_ = self._create_model()\n",
    "        if pretrained:\n",
    "            base_path = '/kaggle/input/model-tabnet/'\n",
    "        else:\n",
    "            base_path = './'\n",
    "        self.estimator_.load_weights(base_path + f'model_tabnet/weights_{repeat}_{fold}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:00:42.347060Z",
     "iopub.status.busy": "2020-11-29T08:00:42.346364Z",
     "iopub.status.idle": "2020-11-29T08:00:42.352776Z",
     "shell.execute_reply": "2020-11-29T08:00:42.352046Z"
    },
    "papermill": {
     "duration": 0.07335,
     "end_time": "2020-11-29T08:00:42.352908",
     "exception": false,
     "start_time": "2020-11-29T08:00:42.279558",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1217, 231, 902, 3893, 111]\n"
     ]
    }
   ],
   "source": [
    "n_repeats = 5\n",
    "n_folds = 7\n",
    "kfold_seeds = [1217, 231, 902, 3893, 111] if REUSE_SPLITS_TABNET else np.random.randint(42, 1337, n_repeats)\n",
    "kfold_seeds = kfold_seeds[:n_repeats]\n",
    "assert len(kfold_seeds) == n_repeats\n",
    "print(kfold_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:00:42.488452Z",
     "iopub.status.busy": "2020-11-29T08:00:42.483123Z",
     "iopub.status.idle": "2020-11-29T08:08:31.269114Z",
     "shell.execute_reply": "2020-11-29T08:08:31.268218Z"
    },
    "papermill": {
     "duration": 468.855813,
     "end_time": "2020-11-29T08:08:31.269246",
     "exception": false,
     "start_time": "2020-11-29T08:00:42.413433",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Repeat 1/5. Fold 1/7\n",
      "Val loss: 0.015734296292066574\n",
      "** Repeat 1/5. Fold 2/7\n",
      "Val loss: 0.015842722728848457\n",
      "** Repeat 1/5. Fold 3/7\n",
      "Val loss: 0.016170810908079147\n",
      "** Repeat 1/5. Fold 4/7\n",
      "Val loss: 0.015860486775636673\n",
      "** Repeat 1/5. Fold 5/7\n",
      "Val loss: 0.01592175103724003\n",
      "** Repeat 1/5. Fold 6/7\n",
      "Val loss: 0.01600259356200695\n",
      "** Repeat 1/5. Fold 7/7\n",
      "Val loss: 0.016120461747050285\n",
      "----------\n",
      "Repeat avg val loss: 0.01595044694840908\n",
      "Repeat OOF val loss: 0.01595043474063799\n",
      "----------\n",
      "** Repeat 2/5. Fold 1/7\n",
      "Val loss: 0.015831606462597847\n",
      "** Repeat 2/5. Fold 2/7\n",
      "Val loss: 0.015790700912475586\n",
      "** Repeat 2/5. Fold 3/7\n",
      "Val loss: 0.016097530722618103\n",
      "** Repeat 2/5. Fold 4/7\n",
      "Val loss: 0.015869125723838806\n",
      "** Repeat 2/5. Fold 5/7\n",
      "Val loss: 0.016081590205430984\n",
      "** Repeat 2/5. Fold 6/7\n",
      "Val loss: 0.016186177730560303\n",
      "** Repeat 2/5. Fold 7/7\n",
      "Val loss: 0.015976887196302414\n",
      "----------\n",
      "Repeat avg val loss: 0.015976231545209885\n",
      "Repeat OOF val loss: 0.015729820695384318\n",
      "----------\n",
      "** Repeat 3/5. Fold 1/7\n",
      "Val loss: 0.01583823561668396\n",
      "** Repeat 3/5. Fold 2/7\n",
      "Val loss: 0.016002388671040535\n",
      "** Repeat 3/5. Fold 3/7\n",
      "Val loss: 0.01609944924712181\n",
      "** Repeat 3/5. Fold 4/7\n",
      "Val loss: 0.015798650681972504\n",
      "** Repeat 3/5. Fold 5/7\n",
      "Val loss: 0.016018105670809746\n",
      "** Repeat 3/5. Fold 6/7\n",
      "Val loss: 0.015845373272895813\n",
      "** Repeat 3/5. Fold 7/7\n",
      "Val loss: 0.015924783423542976\n",
      "----------\n",
      "Repeat avg val loss: 0.01593242585659027\n",
      "Repeat OOF val loss: 0.015638386068677783\n",
      "----------\n",
      "** Repeat 4/5. Fold 1/7\n",
      "Val loss: 0.015721071511507034\n",
      "** Repeat 4/5. Fold 2/7\n",
      "Val loss: 0.01602945290505886\n",
      "** Repeat 4/5. Fold 3/7\n",
      "Val loss: 0.015837348997592926\n",
      "** Repeat 4/5. Fold 4/7\n",
      "Val loss: 0.01604851335287094\n",
      "** Repeat 4/5. Fold 5/7\n",
      "Val loss: 0.015837421640753746\n",
      "** Repeat 4/5. Fold 6/7\n",
      "Val loss: 0.01608298532664776\n",
      "** Repeat 4/5. Fold 7/7\n",
      "Val loss: 0.01592186465859413\n",
      "----------\n",
      "Repeat avg val loss: 0.01592552289366722\n",
      "Repeat OOF val loss: 0.015596331264515848\n",
      "----------\n",
      "** Repeat 5/5. Fold 1/7\n",
      "Val loss: 0.015911342576146126\n",
      "** Repeat 5/5. Fold 2/7\n",
      "Val loss: 0.01593448594212532\n",
      "** Repeat 5/5. Fold 3/7\n",
      "Val loss: 0.01580958627164364\n",
      "** Repeat 5/5. Fold 4/7\n",
      "Val loss: 0.016173483803868294\n",
      "** Repeat 5/5. Fold 5/7\n",
      "Val loss: 0.015941990539431572\n",
      "** Repeat 5/5. Fold 6/7\n",
      "Val loss: 0.016153044998645782\n",
      "** Repeat 5/5. Fold 7/7\n",
      "Val loss: 0.015630777925252914\n",
      "----------\n",
      "Repeat avg val loss: 0.015936387702822685\n",
      "Repeat OOF val loss: 0.015571705744568199\n",
      "----------\n",
      "==========\n",
      "Overall avg val loss: 0.01594420336186886\n",
      "Overall OOF val loss: 0.015571705744568199\n"
     ]
    }
   ],
   "source": [
    "val_losses = []\n",
    "histories = []\n",
    "Y_train_preds_tabnet = pd.DataFrame(np.zeros((X_train.shape[0], num_labels)), columns=label_cols)\n",
    "Y_test_preds_tabnet = pd.DataFrame(np.zeros((X_test_full.shape[0], num_labels)), columns=label_cols)\n",
    "\n",
    "set_random_seeds(888)\n",
    "\n",
    "for repeat, kf_seed in enumerate(kfold_seeds):\n",
    "    kf = MultilabelStratifiedKFold(n_splits=n_folds, random_state=kf_seed, shuffle=True)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X_train_tabnet, Y_train)):\n",
    "        print('** Repeat {}/{}. Fold {}/{}'.format(repeat + 1, n_repeats, fold + 1, n_folds))\n",
    "        K.clear_session()\n",
    "        train_idx, test_idx = complete_train_labels(Y_train, train_idx, test_idx)\n",
    "        train_idx = tf.random.shuffle(train_idx)\n",
    "        X_train_fold = tf.gather(X_train_tabnet, train_idx, axis=0)\n",
    "        Y_train_fold = tf.gather(Y_train, train_idx, axis=0)\n",
    "        X_valid_fold = tf.gather(X_train_tabnet, test_idx, axis=0)\n",
    "        Y_valid_fold = tf.gather(Y_train, test_idx, axis=0)\n",
    "\n",
    "        model_tabnet = MultiLabelTabNet(\n",
    "            n_input_features=X_train_tabnet.shape[1],\n",
    "            n_output_labels=Y_train.shape[1]\n",
    "        )\n",
    "        if LOAD_PRETRAINED_TABNET:\n",
    "            model_tabnet.load_weights(repeat, fold)\n",
    "            val_loss = model_tabnet.score(X_valid_fold, Y_valid_fold.numpy())\n",
    "        else:\n",
    "            history = model_tabnet.fit(\n",
    "                X_train_fold, Y_train_fold,\n",
    "                X_valid_fold, Y_valid_fold,\n",
    "                repeat, fold\n",
    "            )\n",
    "            histories.append(history)\n",
    "            val_loss = min(history.history['val_binary_crossentropy'])\n",
    "            model_tabnet.load_weights(repeat, fold, pretrained=False)\n",
    "\n",
    "        val_preds = model_tabnet.predict(X_valid_fold)\n",
    "        add_outputs(Y_train_preds_tabnet, val_preds, idx_list=test_idx)\n",
    "\n",
    "        test_preds = model_tabnet.predict(X_test_tabnet)\n",
    "        add_outputs(Y_test_preds_tabnet, test_preds, X=X_test_full)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        print('Val loss: {}'.format(val_loss))\n",
    "        \n",
    "        del model_tabnet\n",
    "\n",
    "    print('----------')\n",
    "    print('Repeat avg val loss: {}'.format(np.mean(val_losses[repeat*n_folds:((repeat + 1)*n_folds)])))\n",
    "    print('Repeat OOF val loss: {}'.format(logloss(Y_train, Y_train_preds_tabnet / (repeat + 1), remove_unpredicted=True)))\n",
    "    print('----------')\n",
    "\n",
    "Y_train_preds_tabnet /= n_repeats\n",
    "Y_test_preds_tabnet.loc[X_test_full['cp_type'] == 'trt_cp', :] /= (n_folds * n_repeats)\n",
    "print('==========')\n",
    "print('Overall avg val loss: {}'.format(np.mean(val_losses)))\n",
    "print('Overall OOF val loss: {}'.format(logloss(Y_train, Y_train_preds_tabnet, remove_unpredicted=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:08:31.500151Z",
     "iopub.status.busy": "2020-11-29T08:08:31.499337Z",
     "iopub.status.idle": "2020-11-29T08:08:31.503585Z",
     "shell.execute_reply": "2020-11-29T08:08:31.502833Z"
    },
    "papermill": {
     "duration": 0.120935,
     "end_time": "2020-11-29T08:08:31.503696",
     "exception": false,
     "start_time": "2020-11-29T08:08:31.382761",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !zip -r model_tabnet.zip model_tabnet\n",
    "# !rm -r model_tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:08:31.699446Z",
     "iopub.status.busy": "2020-11-29T08:08:31.698562Z",
     "iopub.status.idle": "2020-11-29T08:08:31.703854Z",
     "shell.execute_reply": "2020-11-29T08:08:31.702417Z"
    },
    "papermill": {
     "duration": 0.11276,
     "end_time": "2020-11-29T08:08:31.704022",
     "exception": false,
     "start_time": "2020-11-29T08:08:31.591262",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    df = pd.DataFrame({\n",
    "        'loss': history.history['loss'],\n",
    "        'val_loss': history.history['val_loss'],\n",
    "    })\n",
    "    print(df['val_loss'].min())\n",
    "    df = df.loc[1:, :]\n",
    "    df.plot()\n",
    "    plt.axvline(x=df['val_loss'].idxmin(), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:08:31.940948Z",
     "iopub.status.busy": "2020-11-29T08:08:31.940320Z",
     "iopub.status.idle": "2020-11-29T08:08:31.943035Z",
     "shell.execute_reply": "2020-11-29T08:08:31.942416Z"
    },
    "papermill": {
     "duration": 0.121891,
     "end_time": "2020-11-29T08:08:31.943140",
     "exception": false,
     "start_time": "2020-11-29T08:08:31.821249",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_history(histories[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081167,
     "end_time": "2020-11-29T08:08:32.102228",
     "exception": false,
     "start_time": "2020-11-29T08:08:32.021061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:08:32.306487Z",
     "iopub.status.busy": "2020-11-29T08:08:32.305607Z",
     "iopub.status.idle": "2020-11-29T08:08:32.309477Z",
     "shell.execute_reply": "2020-11-29T08:08:32.308905Z"
    },
    "papermill": {
     "duration": 0.129226,
     "end_time": "2020-11-29T08:08:32.309576",
     "exception": false,
     "start_time": "2020-11-29T08:08:32.180350",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOAD_PRETRAINED_RESNET = True\n",
    "REUSE_SPLITS_RESNET = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:08:32.471360Z",
     "iopub.status.busy": "2020-11-29T08:08:32.470589Z",
     "iopub.status.idle": "2020-11-29T08:08:32.474416Z",
     "shell.execute_reply": "2020-11-29T08:08:32.475036Z"
    },
    "papermill": {
     "duration": 0.08953,
     "end_time": "2020-11-29T08:08:32.475181",
     "exception": false,
     "start_time": "2020-11-29T08:08:32.385651",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_resnet(X):\n",
    "    X_scaled = pd.DataFrame(std_scaler_2.transform(X[numerical_cols]), columns=numerical_cols)\n",
    "    X_genes = gene_pca_2.transform(X_scaled[gene_cols])\n",
    "    X_cells = cell_pca_2.transform(X_scaled[cell_cols])\n",
    "    return select_features(X), np.concatenate([X_genes, X_cells], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:08:32.692819Z",
     "iopub.status.busy": "2020-11-29T08:08:32.641491Z",
     "iopub.status.idle": "2020-11-29T08:08:33.135595Z",
     "shell.execute_reply": "2020-11-29T08:08:33.134772Z"
    },
    "papermill": {
     "duration": 0.584076,
     "end_time": "2020-11-29T08:08:33.135719",
     "exception": false,
     "start_time": "2020-11-29T08:08:32.551643",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_resnet_1, X_train_resnet_2 = preprocess_resnet(X_train)\n",
    "X_test_resnet_1, X_test_resnet_2 = preprocess_resnet(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:08:33.378687Z",
     "iopub.status.busy": "2020-11-29T08:08:33.377839Z",
     "iopub.status.idle": "2020-11-29T08:08:33.735367Z",
     "shell.execute_reply": "2020-11-29T08:08:33.734610Z"
    },
    "papermill": {
     "duration": 0.483766,
     "end_time": "2020-11-29T08:08:33.735478",
     "exception": false,
     "start_time": "2020-11-29T08:08:33.251712",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir model_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:08:33.909244Z",
     "iopub.status.busy": "2020-11-29T08:08:33.903969Z",
     "iopub.status.idle": "2020-11-29T08:08:33.964968Z",
     "shell.execute_reply": "2020-11-29T08:08:33.964429Z"
    },
    "papermill": {
     "duration": 0.150193,
     "end_time": "2020-11-29T08:08:33.965079",
     "exception": false,
     "start_time": "2020-11-29T08:08:33.814886",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLabelResNet(BaseEstimator):\n",
    "\n",
    "    def __init__(self, n_input_1, n_input_2, n_output_labels):\n",
    "        self.n_input_1 = n_input_1\n",
    "        self.n_input_2 = n_input_2\n",
    "        self.n_output_labels = n_output_labels\n",
    "        self.estimator_ = None\n",
    "        \n",
    "    def _create_model(self, initial_bias, loss_fn=None):\n",
    "        output_bias = keras.initializers.Constant(initial_bias)\n",
    "        \n",
    "        inp_1 = keras.layers.Input(self.n_input_1, name='raw_inp')\n",
    "        inp_2 = keras.layers.Input(self.n_input_2, name='pca_inp')\n",
    "        gauss_2 = keras.layers.GaussianNoise(1e-4)(inp_2)\n",
    "\n",
    "        head_1 = keras.models.Sequential([\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.2),\n",
    "            tfa.layers.WeightNormalization(keras.layers.Dense(512, activation='elu')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            tfa.layers.WeightNormalization(keras.layers.Dense(256, activation='elu'))\n",
    "        ], name='head1') \n",
    "\n",
    "        seq_1 = head_1(inp_1)\n",
    "        seq_1_inp_concat = keras.layers.Concatenate()([gauss_2, seq_1])\n",
    "\n",
    "        head_2 = keras.models.Sequential([\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.4),\n",
    "            tfa.layers.WeightNormalization(keras.layers.Dense(512, \"relu\")),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            tfa.layers.WeightNormalization(keras.layers.Dense(512, \"elu\")),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            tfa.layers.WeightNormalization(keras.layers.Dense(256, \"relu\")),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            tfa.layers.WeightNormalization(keras.layers.Dense(256, \"elu\"))\n",
    "        ], name='head2')\n",
    "\n",
    "        seq_2 = head_2(seq_1_inp_concat)\n",
    "        seq_1_seq_2_avg = keras.layers.Average()([seq_1, seq_2]) \n",
    "\n",
    "        head_3 = keras.models.Sequential([\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            tfa.layers.WeightNormalization(keras.layers.Dense(256, activation='relu')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            tfa.layers.WeightNormalization(keras.layers.Dense(206, activation='relu')),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dropout(0.1),\n",
    "            tfa.layers.WeightNormalization(keras.layers.Dense(self.n_output_labels, bias_initializer=output_bias, activation='sigmoid'))\n",
    "        ], name='head3')\n",
    "\n",
    "        output = head_3(seq_1_seq_2_avg)\n",
    "        \n",
    "        model = keras.models.Model(inputs=[inp_1, inp_2], outputs=output)\n",
    "        model.compile(\n",
    "            loss=keras.losses.BinaryCrossentropy(label_smoothing=2e-3) if loss_fn is None else loss_fn,\n",
    "            optimizer=tfa.optimizers.AdamW(learning_rate=0.02, weight_decay=1e-5),\n",
    "            metrics=['binary_crossentropy'],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, Y, X_valid, Y_valid, repeat, fold, max_epochs=100, save_weights=True, initial_bias=initial_bias, transfer_model=None):\n",
    "        Y = tf.cast(Y, tf.float32)\n",
    "        Y_valid = tf.cast(Y_valid, tf.float32)\n",
    "\n",
    "        class_weights = balance_class_weights(Y.numpy(), class_weights=(0.5, 2.0))\n",
    "        loss_fn = weighted_binary_crossentropy(class_weights, label_smoothing=2e-3)\n",
    "        self.estimator_ = self._create_model(initial_bias, loss_fn=loss_fn)\n",
    "        if transfer_model is not None:\n",
    "            self.copy_weights(transfer_model)\n",
    "\n",
    "        learning_rate_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            patience=3, verbose=1, factor=0.1, min_lr=1e-8\n",
    "        )\n",
    "        early_stop_cb = keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.0001, patience=15,\n",
    "            restore_best_weights=False\n",
    "        )\n",
    "        checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "            f'model_resnet/weights_{repeat}_{fold}.h5',\n",
    "            monitor='val_binary_crossentropy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True\n",
    "        )\n",
    "        callbacks = [early_stop_cb, learning_rate_cb, checkpoint_cb] if save_weights else [early_stop_cb, learning_rate_cb]\n",
    "\n",
    "        history = self.estimator_.fit(\n",
    "            X, Y,\n",
    "            batch_size=128,\n",
    "            epochs=max_epochs,\n",
    "            validation_data=(X_valid, Y_valid),\n",
    "            callbacks=callbacks,\n",
    "            shuffle=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.estimator_.predict(X)\n",
    "    \n",
    "    def score(self, X, Y_true):\n",
    "        Y_preds = self.predict(X)\n",
    "        return logloss(Y_true, Y_preds)\n",
    "    \n",
    "    def save_weights(self, repeat, fold):\n",
    "        self.estimator_.save_weights('model_resnet/weights_{}_{}.h5'.format(repeat, fold))\n",
    "        \n",
    "    def load_weights(self, repeat, fold, pretrained=True):\n",
    "        if self.estimator_ is None:\n",
    "            self.estimator_ = self._create_model(initial_bias)\n",
    "        if pretrained:\n",
    "            base_path = '/kaggle/input/model-resnet/'\n",
    "        else:\n",
    "            base_path = './'\n",
    "        self.estimator_.load_weights(base_path + 'model_resnet/weights_{}_{}.h5'.format(repeat, fold))\n",
    "        \n",
    "    def copy_weights(self, from_model):\n",
    "        for to_layer, from_layer in zip(self.estimator_.layers[:-1], from_model.estimator_.layers[:-1]):\n",
    "            to_layer.set_weights(from_layer.get_weights())\n",
    "        for to_layer, from_layer in zip(self.estimator_.layers[-1].layers[:-1], from_model.estimator_.layers[-1].layers[:-1]):\n",
    "            to_layer.set_weights(from_layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:08:34.133275Z",
     "iopub.status.busy": "2020-11-29T08:08:34.132387Z",
     "iopub.status.idle": "2020-11-29T08:08:34.137478Z",
     "shell.execute_reply": "2020-11-29T08:08:34.136729Z"
    },
    "papermill": {
     "duration": 0.092629,
     "end_time": "2020-11-29T08:08:34.137628",
     "exception": false,
     "start_time": "2020-11-29T08:08:34.044999",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[188, 526, 1045, 453, 200, 8684, 87352]\n"
     ]
    }
   ],
   "source": [
    "n_repeats = 7\n",
    "n_folds = 7\n",
    "kfold_seeds = [188, 526, 1045, 453, 200, 8684, 87352] if REUSE_SPLITS_RESNET else np.random.randint(42, 1337, n_repeats)\n",
    "kfold_seeds = kfold_seeds[:n_repeats]\n",
    "assert len(kfold_seeds) == n_repeats\n",
    "print(kfold_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:08:34.336888Z",
     "iopub.status.busy": "2020-11-29T08:08:34.331784Z",
     "iopub.status.idle": "2020-11-29T08:14:04.671818Z",
     "shell.execute_reply": "2020-11-29T08:14:04.671284Z"
    },
    "papermill": {
     "duration": 330.441874,
     "end_time": "2020-11-29T08:14:04.671926",
     "exception": false,
     "start_time": "2020-11-29T08:08:34.230052",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Repeat 1/7. Fold 1/7\n",
      "Val loss: 0.01573297567665577\n",
      "** Repeat 1/7. Fold 2/7\n",
      "Val loss: 0.016409706324338913\n",
      "** Repeat 1/7. Fold 3/7\n",
      "Val loss: 0.01591956429183483\n",
      "** Repeat 1/7. Fold 4/7\n",
      "Val loss: 0.015869781374931335\n",
      "** Repeat 1/7. Fold 5/7\n",
      "Val loss: 0.015978172421455383\n",
      "** Repeat 1/7. Fold 6/7\n",
      "Val loss: 0.015889571979641914\n",
      "** Repeat 1/7. Fold 7/7\n",
      "Val loss: 0.016222050413489342\n",
      "----------\n",
      "Repeat avg val loss: 0.016003118827939034\n",
      "Repeat OOF val loss: 0.01600313718700611\n",
      "----------\n",
      "** Repeat 2/7. Fold 1/7\n",
      "Val loss: 0.016249777749180794\n",
      "** Repeat 2/7. Fold 2/7\n",
      "Val loss: 0.01603727973997593\n",
      "** Repeat 2/7. Fold 3/7\n",
      "Val loss: 0.015846941620111465\n",
      "** Repeat 2/7. Fold 4/7\n",
      "Val loss: 0.016024982556700706\n",
      "** Repeat 2/7. Fold 5/7\n",
      "Val loss: 0.01614505425095558\n",
      "** Repeat 2/7. Fold 6/7\n",
      "Val loss: 0.01601618528366089\n",
      "** Repeat 2/7. Fold 7/7\n",
      "Val loss: 0.015901396051049232\n",
      "----------\n",
      "Repeat avg val loss: 0.0160316601395607\n",
      "Repeat OOF val loss: 0.015883193546596\n",
      "----------\n",
      "** Repeat 3/7. Fold 1/7\n",
      "Val loss: 0.01596708968281746\n",
      "** Repeat 3/7. Fold 2/7\n",
      "Val loss: 0.015866534784436226\n",
      "** Repeat 3/7. Fold 3/7\n",
      "Val loss: 0.016084380447864532\n",
      "** Repeat 3/7. Fold 4/7\n",
      "Val loss: 0.015988370403647423\n",
      "** Repeat 3/7. Fold 5/7\n",
      "Val loss: 0.015949606895446777\n",
      "** Repeat 3/7. Fold 6/7\n",
      "Val loss: 0.016028016805648804\n",
      "** Repeat 3/7. Fold 7/7\n",
      "Val loss: 0.016022006049752235\n",
      "----------\n",
      "Repeat avg val loss: 0.015986572951078415\n",
      "Repeat OOF val loss: 0.0158232756240678\n",
      "----------\n",
      "** Repeat 4/7. Fold 1/7\n",
      "Val loss: 0.016196545213460922\n",
      "** Repeat 4/7. Fold 2/7\n",
      "Val loss: 0.016243426129221916\n",
      "** Repeat 4/7. Fold 3/7\n",
      "Val loss: 0.015956146642565727\n",
      "** Repeat 4/7. Fold 4/7\n",
      "Val loss: 0.01596860960125923\n",
      "** Repeat 4/7. Fold 5/7\n",
      "Val loss: 0.015980152413249016\n",
      "** Repeat 4/7. Fold 6/7\n",
      "Val loss: 0.015959467738866806\n",
      "** Repeat 4/7. Fold 7/7\n",
      "Val loss: 0.015936018899083138\n",
      "----------\n",
      "Repeat avg val loss: 0.016034338623285294\n",
      "Repeat OOF val loss: 0.01580515822859084\n",
      "----------\n",
      "** Repeat 5/7. Fold 1/7\n",
      "Val loss: 0.016010450199246407\n",
      "** Repeat 5/7. Fold 2/7\n",
      "Val loss: 0.016241462901234627\n",
      "** Repeat 5/7. Fold 3/7\n",
      "Val loss: 0.016008026897907257\n",
      "** Repeat 5/7. Fold 4/7\n",
      "Val loss: 0.01591651141643524\n",
      "** Repeat 5/7. Fold 5/7\n",
      "Val loss: 0.016015946865081787\n",
      "** Repeat 5/7. Fold 6/7\n",
      "Val loss: 0.01622123084962368\n",
      "** Repeat 5/7. Fold 7/7\n",
      "Val loss: 0.015876539051532745\n",
      "----------\n",
      "Repeat avg val loss: 0.016041453927755356\n",
      "Repeat OOF val loss: 0.01579609205164187\n",
      "----------\n",
      "** Repeat 6/7. Fold 1/7\n",
      "Val loss: 0.01615903340280056\n",
      "** Repeat 6/7. Fold 2/7\n",
      "Val loss: 0.015923066064715385\n",
      "** Repeat 6/7. Fold 3/7\n",
      "Val loss: 0.015801794826984406\n",
      "** Repeat 6/7. Fold 4/7\n",
      "Val loss: 0.016093440353870392\n",
      "** Repeat 6/7. Fold 5/7\n",
      "Val loss: 0.015723681077361107\n",
      "** Repeat 6/7. Fold 6/7\n",
      "Val loss: 0.016182277351617813\n",
      "** Repeat 6/7. Fold 7/7\n",
      "Val loss: 0.01596720516681671\n",
      "----------\n",
      "Repeat avg val loss: 0.01597864367067814\n",
      "Repeat OOF val loss: 0.015776885536039866\n",
      "----------\n",
      "** Repeat 7/7. Fold 1/7\n",
      "Val loss: 0.016074007377028465\n",
      "** Repeat 7/7. Fold 2/7\n",
      "Val loss: 0.016040200367569923\n",
      "** Repeat 7/7. Fold 3/7\n",
      "Val loss: 0.016199564561247826\n",
      "** Repeat 7/7. Fold 4/7\n",
      "Val loss: 0.016152208670973778\n",
      "** Repeat 7/7. Fold 5/7\n",
      "Val loss: 0.01598931849002838\n",
      "** Repeat 7/7. Fold 6/7\n",
      "Val loss: 0.01622510328888893\n",
      "** Repeat 7/7. Fold 7/7\n",
      "Val loss: 0.01584501564502716\n",
      "----------\n",
      "Repeat avg val loss: 0.01607505977153778\n",
      "Repeat OOF val loss: 0.015783340559348803\n",
      "----------\n",
      "==========\n",
      "Overall avg val loss: 0.016021547839045525\n",
      "Overall OOF val loss: 0.015783340559348803\n"
     ]
    }
   ],
   "source": [
    "val_losses = []\n",
    "histories = []\n",
    "Y_train_preds_resnet = pd.DataFrame(np.zeros((X_train.shape[0], num_labels)), columns=label_cols)\n",
    "Y_test_preds_resnet = pd.DataFrame(np.zeros((X_test_full.shape[0], num_labels)), columns=label_cols)\n",
    "\n",
    "set_random_seeds(123)\n",
    "\n",
    "for repeat, kf_seed in enumerate(kfold_seeds):\n",
    "    kf = MultilabelStratifiedKFold(n_splits=n_folds, random_state=kf_seed, shuffle=True)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X_train, Y_train)):\n",
    "        print('** Repeat {}/{}. Fold {}/{}'.format(repeat + 1, n_repeats, fold + 1, n_folds))\n",
    "        K.clear_session()\n",
    "        train_idx, test_idx = complete_train_labels(Y_train, train_idx, test_idx)\n",
    "        train_idx = tf.random.shuffle(train_idx)\n",
    "        X_train_1_fold = tf.gather(X_train_resnet_1, train_idx, axis=0)\n",
    "        X_train_2_fold = tf.gather(X_train_resnet_2, train_idx, axis=0)\n",
    "        Y_train_fold = tf.gather(Y_train, train_idx, axis=0)\n",
    "        X_valid_1_fold = tf.gather(X_train_resnet_1, test_idx, axis=0)\n",
    "        X_valid_2_fold = tf.gather(X_train_resnet_2, test_idx, axis=0)\n",
    "        Y_valid_fold = tf.gather(Y_train, test_idx, axis=0)\n",
    "        Y_nonscored_train_fold = tf.gather(Y_train_nonscored, train_idx, axis=0)\n",
    "        Y_nonscored_valid_fold = tf.gather(Y_train_nonscored, test_idx, axis=0)\n",
    "\n",
    "        model_resnet = MultiLabelResNet(\n",
    "            n_input_1=X_train_resnet_1.shape[1],\n",
    "            n_input_2=X_train_resnet_2.shape[1],\n",
    "            n_output_labels=Y_train.shape[1]\n",
    "        )\n",
    "        if LOAD_PRETRAINED_RESNET:\n",
    "            model_resnet.load_weights(repeat, fold)\n",
    "            val_loss = model_resnet.score([X_valid_1_fold, X_valid_2_fold], Y_valid_fold.numpy())\n",
    "        else:\n",
    "            print('Pretrain model')\n",
    "            transfer_model = MultiLabelResNet(\n",
    "                n_input_1=X_train_resnet_1.shape[1],\n",
    "                n_input_2=X_train_resnet_2.shape[1],\n",
    "                n_output_labels=Y_train_nonscored.shape[1]\n",
    "            )\n",
    "            transfer_model.fit(\n",
    "                [X_train_1_fold, X_train_2_fold], Y_nonscored_train_fold,\n",
    "                [X_valid_1_fold, X_valid_2_fold], Y_nonscored_valid_fold,\n",
    "                repeat, fold,\n",
    "                max_epochs=10,\n",
    "                save_weights=False,\n",
    "                initial_bias=nonscored_initial_bias\n",
    "            )\n",
    "\n",
    "            print('Train model')\n",
    "            history = model_resnet.fit(\n",
    "                [X_train_1_fold, X_train_2_fold], Y_train_fold,\n",
    "                [X_valid_1_fold, X_valid_2_fold], Y_valid_fold,\n",
    "                repeat, fold,\n",
    "                transfer_model=transfer_model\n",
    "            ) \n",
    "            histories.append(history)\n",
    "            val_loss = min(history.history['val_binary_crossentropy'])\n",
    "            model_resnet.load_weights(repeat, fold, pretrained=False)\n",
    "            del transfer_model\n",
    "\n",
    "        val_preds = model_resnet.predict([X_valid_1_fold, X_valid_2_fold])\n",
    "        add_outputs(Y_train_preds_resnet, val_preds, idx_list=test_idx)\n",
    "\n",
    "        test_preds = model_resnet.predict([X_test_resnet_1, X_test_resnet_2])\n",
    "        add_outputs(Y_test_preds_resnet, test_preds, X=X_test_full)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        print('Val loss: {}'.format(val_loss))\n",
    "\n",
    "        del model_resnet\n",
    "    \n",
    "    print('----------')\n",
    "    print('Repeat avg val loss: {}'.format(np.mean(val_losses[repeat*n_folds:((repeat + 1)*n_folds)])))\n",
    "    print('Repeat OOF val loss: {}'.format(logloss(Y_train, Y_train_preds_resnet / (repeat + 1), remove_unpredicted=True)))\n",
    "    print('----------')\n",
    "\n",
    "Y_train_preds_resnet /= n_repeats       \n",
    "Y_test_preds_resnet.loc[X_test_full['cp_type'] == 'trt_cp', :] /= (n_folds * n_repeats)\n",
    "print('==========')\n",
    "print('Overall avg val loss: {}'.format(np.mean(val_losses)))\n",
    "print('Overall OOF val loss: {}'.format(logloss(Y_train, Y_train_preds_resnet, remove_unpredicted=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:14:04.871384Z",
     "iopub.status.busy": "2020-11-29T08:14:04.870691Z",
     "iopub.status.idle": "2020-11-29T08:14:04.874571Z",
     "shell.execute_reply": "2020-11-29T08:14:04.873827Z"
    },
    "papermill": {
     "duration": 0.105441,
     "end_time": "2020-11-29T08:14:04.874683",
     "exception": false,
     "start_time": "2020-11-29T08:14:04.769242",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !zip -r model_resnet.zip model_resnet\n",
    "# !rm -r model_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:14:05.079086Z",
     "iopub.status.busy": "2020-11-29T08:14:05.078447Z",
     "iopub.status.idle": "2020-11-29T08:14:05.082053Z",
     "shell.execute_reply": "2020-11-29T08:14:05.081420Z"
    },
    "papermill": {
     "duration": 0.109455,
     "end_time": "2020-11-29T08:14:05.082160",
     "exception": false,
     "start_time": "2020-11-29T08:14:04.972705",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    df = pd.DataFrame({\n",
    "        'loss': history.history['binary_crossentropy'],\n",
    "        'val_loss': history.history['val_binary_crossentropy'],\n",
    "    })\n",
    "    print(df['val_loss'].min())\n",
    "    df = df.loc[1:, :]\n",
    "    df.plot()\n",
    "    plt.axvline(x=df['val_loss'].idxmin(), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:14:05.281226Z",
     "iopub.status.busy": "2020-11-29T08:14:05.280637Z",
     "iopub.status.idle": "2020-11-29T08:14:05.284410Z",
     "shell.execute_reply": "2020-11-29T08:14:05.283748Z"
    },
    "papermill": {
     "duration": 0.104753,
     "end_time": "2020-11-29T08:14:05.284510",
     "exception": false,
     "start_time": "2020-11-29T08:14:05.179757",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_history(histories[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.098013,
     "end_time": "2020-11-29T08:14:05.479094",
     "exception": false,
     "start_time": "2020-11-29T08:14:05.381081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:14:05.679194Z",
     "iopub.status.busy": "2020-11-29T08:14:05.678578Z",
     "iopub.status.idle": "2020-11-29T08:14:05.682099Z",
     "shell.execute_reply": "2020-11-29T08:14:05.681494Z"
    },
    "papermill": {
     "duration": 0.105438,
     "end_time": "2020-11-29T08:14:05.682197",
     "exception": false,
     "start_time": "2020-11-29T08:14:05.576759",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_weights = balance_class_weights(Y_train.values, class_weights=(0.5, 10.0))\n",
    "# loss_fn = weighted_binary_crossentropy(class_weights, label_smoothing=2e-3)\n",
    "\n",
    "# def objective_func(Y_true, Y_preds_list, weights):\n",
    "#     blended_preds = sum([w*Y_preds for w, Y_preds in zip(weights, Y_preds_list)])\n",
    "#     penalty = 1e-3*(sum(weights) - 1)**2\n",
    "#     if min(weights) < 0:\n",
    "#         penalty += -10*min(weights)\n",
    "#     return tf.reduce_mean(loss_fn(Y_true, blended_preds)) + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:14:05.884471Z",
     "iopub.status.busy": "2020-11-29T08:14:05.883797Z",
     "iopub.status.idle": "2020-11-29T08:14:05.887166Z",
     "shell.execute_reply": "2020-11-29T08:14:05.886616Z"
    },
    "papermill": {
     "duration": 0.107533,
     "end_time": "2020-11-29T08:14:05.887320",
     "exception": false,
     "start_time": "2020-11-29T08:14:05.779787",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init_weights = [0.22910333, 0.41949411, 0.35140255]\n",
    "# print(sum(init_weights))\n",
    "# assert abs(sum(init_weights) - 1) < 1e-6\n",
    "# preds_list = [Y_train_preds_nn, Y_train_preds_tabnet, Y_train_preds_resnet]\n",
    "# val_blended_preds = pd.DataFrame(np.zeros((X_train.shape[0], num_labels)), columns=label_cols)\n",
    "\n",
    "# losses = []\n",
    "# val_losses = []\n",
    "# all_losses = []\n",
    "# all_weight_vars = []\n",
    "# num_epochs = 300\n",
    "\n",
    "# kf = MultilabelStratifiedKFold(n_splits=5, random_state=34, shuffle=True)\n",
    "# for fold, (train_idx, test_idx) in enumerate(kf.split(np.zeros(X_train.shape), Y_train)):\n",
    "#     weight_vars = [tf.Variable(w, dtype=tf.float32) for w in init_weights]\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "#     train_idx, test_idx = complete_train_labels(Y_train, train_idx, test_idx)\n",
    "#     tf_Y_train = tf.constant(Y_train.iloc[train_idx, :], dtype=tf.float32)\n",
    "#     tf_preds_list = [tf.constant(y.iloc[train_idx, :], dtype=tf.float32) for y in preds_list]\n",
    "#     tf_Y_valid = tf.constant(Y_train.iloc[test_idx, :], dtype=tf.float32)\n",
    "#     tf_preds_list_valid = [tf.constant(y.iloc[test_idx, :], dtype=tf.float32) for y in preds_list]\n",
    "#     fold_losses = []\n",
    "#     fold_val_losses = []\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             loss_value = objective_func(tf_Y_train, tf_preds_list, weight_vars)\n",
    "#         grads = tape.gradient(loss_value, weight_vars)\n",
    "#         optimizer.apply_gradients(zip(grads, weight_vars))\n",
    "#         print(fold, epoch, loss_value.numpy())\n",
    "#         fold_losses.append(loss_value.numpy())\n",
    "#         fold_val_losses.append(objective_func(tf_Y_valid, tf_preds_list_valid, weight_vars).numpy())\n",
    "\n",
    "#     all_losses.append([fold_losses, fold_val_losses])\n",
    "\n",
    "#     losses.append(loss_value.numpy())\n",
    "#     val_loss = objective_func(tf_Y_valid, tf_preds_list_valid, weight_vars).numpy()\n",
    "#     val_losses.append(val_loss)\n",
    "# #     print(fold, loss_value.numpy(), val_loss)\n",
    "    \n",
    "#     optimized_weights = np.array([v.numpy() for v in weight_vars], dtype=np.float64)\n",
    "#     optimized_weights /= np.sum(optimized_weights)\n",
    "#     all_weight_vars.append(optimized_weights)\n",
    "#     print(optimized_weights)\n",
    "    \n",
    "#     val_preds = sum([w*Y_preds for w, Y_preds in zip(optimized_weights, preds_list)])\n",
    "#     add_outputs(val_blended_preds, val_preds, idx_list=test_idx)\n",
    "    \n",
    "# print(np.mean(losses), np.mean(val_losses), logloss(Y_train, val_blended_preds), np.array(all_weight_vars).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:14:06.087798Z",
     "iopub.status.busy": "2020-11-29T08:14:06.087165Z",
     "iopub.status.idle": "2020-11-29T08:14:06.090616Z",
     "shell.execute_reply": "2020-11-29T08:14:06.089973Z"
    },
    "papermill": {
     "duration": 0.105012,
     "end_time": "2020-11-29T08:14:06.090720",
     "exception": false,
     "start_time": "2020-11-29T08:14:05.985708",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt_fold = 2\n",
    "# plt.plot(all_losses[plt_fold][0])\n",
    "# plt.plot(all_losses[plt_fold][1])\n",
    "# plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:14:06.297134Z",
     "iopub.status.busy": "2020-11-29T08:14:06.295981Z",
     "iopub.status.idle": "2020-11-29T08:14:06.503147Z",
     "shell.execute_reply": "2020-11-29T08:14:06.503977Z"
    },
    "papermill": {
     "duration": 0.315447,
     "end_time": "2020-11-29T08:14:06.504196",
     "exception": false,
     "start_time": "2020-11-29T08:14:06.188749",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.015436135491819642\n"
     ]
    }
   ],
   "source": [
    "weights = [0.22, 0.43, 0.35]\n",
    "print(sum(weights))\n",
    "assert(abs(sum(weights) - 1) < 1e-6)\n",
    "\n",
    "all_train_preds = (\n",
    "    Y_train_preds_nn * weights[0] +\n",
    "    Y_train_preds_tabnet * weights[1] +\n",
    "    Y_train_preds_resnet * weights[2]\n",
    ")\n",
    "print(logloss(Y_train, all_train_preds, remove_unpredicted=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:14:06.808220Z",
     "iopub.status.busy": "2020-11-29T08:14:06.807349Z",
     "iopub.status.idle": "2020-11-29T08:14:06.810727Z",
     "shell.execute_reply": "2020-11-29T08:14:06.810058Z"
    },
    "papermill": {
     "duration": 0.155896,
     "end_time": "2020-11-29T08:14:06.810842",
     "exception": false,
     "start_time": "2020-11-29T08:14:06.654946",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all_train_preds.to_csv('train_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.098484,
     "end_time": "2020-11-29T08:14:07.011472",
     "exception": false,
     "start_time": "2020-11-29T08:14:06.912988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:14:07.221263Z",
     "iopub.status.busy": "2020-11-29T08:14:07.220658Z",
     "iopub.status.idle": "2020-11-29T08:14:07.231885Z",
     "shell.execute_reply": "2020-11-29T08:14:07.232414Z"
    },
    "papermill": {
     "duration": 0.122286,
     "end_time": "2020-11-29T08:14:07.232560",
     "exception": false,
     "start_time": "2020-11-29T08:14:07.110274",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_preds = (\n",
    "    Y_test_preds_nn * weights[0] +\n",
    "    Y_test_preds_tabnet * weights[1] +\n",
    "    Y_test_preds_resnet * weights[2]\n",
    ")\n",
    "final_preds.loc[X_test_full['cp_type'] != 'trt_cp', :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-29T08:14:07.441102Z",
     "iopub.status.busy": "2020-11-29T08:14:07.440219Z",
     "iopub.status.idle": "2020-11-29T08:14:08.782862Z",
     "shell.execute_reply": "2020-11-29T08:14:08.781977Z"
    },
    "papermill": {
     "duration": 1.449301,
     "end_time": "2020-11-29T08:14:08.782993",
     "exception": false,
     "start_time": "2020-11-29T08:14:07.333692",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_preds = np.concatenate([sig_id, final_preds], axis=1)\n",
    "final_preds = pd.DataFrame(final_preds, columns=Y_scored.columns)\n",
    "final_preds.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1096.127701,
   "end_time": "2020-11-29T08:14:09.041599",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-29T07:55:52.913898",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
